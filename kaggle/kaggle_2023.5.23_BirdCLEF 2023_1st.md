
| ç«žèµ›å¹³å° | kaggle                                                       |
| -------- | ------------------------------------------------------------ |
| ç«žèµ›åç§° | [BirdCLEF 2023]([https://www.kaggle.com/competitions/llm-detect-ai-generated-text/leaderboard](https://www.kaggle.com/competitions/birdclef-2023/overview)) |
| ç«žèµ›æŽ’å | 1st                                                          |
| ç«žèµ›æ ‡ç­¾ | Multiclass Classification Audio                       |
| æ–¹æ¡ˆåœ°å€ | https://www.kaggle.com/competitions/birdclef-2023/discussion/412808 https://www.kaggle.com/code/vladimirsydor/bird-clef-2023-inference-v1/notebook |

## ðŸ¤–ðŸ¤–æˆ‘çš„æ”¶èŽ·
å¤§å¸ˆå‚…ä¼¼çš„

## æ­£ç¡®çš„æ•°æ®å°±æ˜¯æ‚¨æ‰€éœ€è¦çš„
## æ•°æ®ï¼Œæ•°æ®æ— å¤„ä¸åœ¨
è®©æˆ‘ä»¬ä»Ž 2023 å¹´çš„è®­ç»ƒæ•°æ®å¼€å§‹

å¦‚æžœä½ çœ‹ä¸€ä¸‹ `train_metadata["primary_label"].value_counts()`ï¼Œä½ å¯èƒ½ä¼šæ³¨æ„åˆ°ä¸€äº›å¥‡æ€ªçš„æœ€å¤§å¹»æ•°ï¼š

```python
barswa     500
wlwwar     500
thrnig1    500
eaywag1    500
comsan     500
          ... 
lotcor1      1
whctur2      1
whhsaw1      1
afpkin1      1
crefra2      1
Name: primary_label, Length: 264, dtype: int64
```
ä¸ºä»€ä¹ˆæŸäº›ç‰©ç§çš„ä»£è¡¨æœ€å¤šæœ‰ 500 ä¸ªï¼Ÿæˆ‘ä¸çŸ¥é“ 100% çš„ç­”æ¡ˆï¼Œä½†æˆ‘æœ‰ä¸€ä¸ªå¼ºæœ‰åŠ›çš„å‡è®¾ - XC API ä¸­çš„é”™è¯¯ã€‚æˆ‘ä¸è®°å¾—ä»£ç ä¸­çš„ç¡®åˆ‡ä½ç½®ï¼Œä½†æ€»ä½“é—®é¢˜åœ¨äºŽæ•°æ®åŠ è½½ç®¡é“ã€‚å®ƒçš„å·¥ä½œåŽŸç†å¦‚ä¸‹ï¼š

1. ä¸‹è½½å…ƒæ–‡ä»¶ - json æ–‡ä»¶ã€‚
2. è¿­ä»£å…ƒæ–‡ä»¶ä¸­çš„æ‰€æœ‰ url å¹¶ä¸‹è½½å®ƒä»¬ã€‚

ä½†æ˜¯ï¼Œå¦‚æžœä¸€ä¸ªç‰©ç§æœ‰è¶…è¿‡ 500 ä¸ªæ–‡ä»¶ - åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæ‚¨å°†æœ‰å‡ ä¸ª json æ–‡ä»¶ï¼ˆä¸€ä¸ª json å›¾å…ƒæ–‡ä»¶ä¸­çš„æœ€å¤§æ–‡ä»¶æ•° = 500ï¼‰ï¼Œè¿™æ—¶å€™æˆ‘ä»¬å°±é‡åˆ°äº†é—®é¢˜ï¼åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒAPI ä»…è€ƒè™‘æ¯ä¸ªç‰©ç§çš„ä¸€ä¸ª jsonï¼Œå¹¶å¿½ç•¥æŽ¥ä¸‹æ¥çš„ jsonï¼Œå› æ­¤æ¯ä¸ªç‰©ç§æœ€å¤šæœ‰ 500 ä¸ªæ–‡ä»¶ã€‚

æ³¨æ„ï¼šæˆ‘ä¸ç¡®å®šå®ƒæ˜¯å¦åœ¨æœ€æ–°ç‰ˆæœ¬çš„ API ä¸­å¾—åˆ°ä¿®å¤ï¼Œä½†æˆ‘ä½¿ç”¨äº†å‰ä¸€å¹´çš„æäº¤ï¼Œè€Œä¸”å®ƒå°±åœ¨é‚£é‡Œã€‚

ä»Žè¿™ä¸ªbugä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°äº†è§£åˆ°ä½¿ç”¨å›ºå®šçš„APIå¯ä»¥æžå¤§åœ°ä¸°å¯Œæˆ‘ä»¬çš„è®­ç»ƒæ•°æ®é›†ã€‚

å…¶ä»–æ¯”è¾ƒæ— èŠçš„äº‹æƒ…
- 2023/2022/2021/2020æ¯”èµ›æ•°æ®
- 2020å¹´é™„åŠ æ¯”èµ›æ•°æ®
- æ³½è¯ºå¤š
- å¼‚æ›²

## é¢å¤–çš„è®­ç»ƒæ•°æ®

ä»Ž2023/2022/2021/2020çš„æ¯”èµ›æ•°æ®åŠ ä¸ŠXeno-Cantoæ•°æ®ä¸­ï¼Œæˆ‘åªé€‰æ‹©äº†å¸¦æœ‰ä»Šå¹´ä¸»è¦æ ‡ç­¾çš„æ–‡ä»¶ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ°è®­ç»ƒçš„æœ€åŽé˜¶æ®µã€‚

## é¢„è®­ç»ƒæ•°æ®é›†

å½“æˆ‘åœ¨è®­ç»ƒçš„æœ€åŽé˜¶æ®µåªä½¿ç”¨2023å¹´çš„è®­ç»ƒæ•°æ®æ—¶ï¼Œå¯¹2022/2021/2020æ¯”èµ›æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œåˆ†æ•°æå‡äº†å¾ˆå¤šã€‚ä½†åœ¨æ·»åŠ é¢å¤–çš„è®­ç»ƒæ•°æ®åŽï¼Œé¢„è®­ç»ƒåœ¨æŽ’è¡Œæ¦œä¸Šåœæ­¢äº†å·¥ä½œï¼ˆå°½ç®¡å®ƒä»ç„¶å¢žåŠ äº†æœ¬åœ°éªŒè¯ï¼‰ã€‚ä¸Šå‘¨ï¼Œæˆ‘å†³å®šé‡æ–°å¼€å§‹é¢„è®­ç»ƒå®žéªŒã€‚è¿™è®©æˆ‘åœ¨å…¬å…±æŽ’è¡Œæ¦œä¸Šä¸Šå‡äº†ä¸€ä½ï¼Œåœ¨ç§äººæŽ’è¡Œæ¦œä¸Šä¸Šå‡äº†ä¸¤ä½ - æ‰€ä»¥ï¼ŒKagglersï¼Œä¸è¦å¿˜è®°é‡æ–°å®¡è§†ç”šè‡³è¢«æ‹’ç»çš„å‡è®¾:)

ä¸ºä»€ä¹ˆä»¥åŠä½•æ—¶èµ·ä½œç”¨ï¼Ÿä¸Žä¹‹å‰çš„é¢„è®­ç»ƒå®žéªŒç›¸æ¯”ï¼Œæˆ‘æœ‰ï¼š

- ä¸ä»…æŒ‰ id è¿‡æ»¤æŽ‰ 2023 ä¸ªç«è½¦æ•°æ®é‡å¤é¡¹ï¼Œè¿˜æŒ‰â€œauthor + Primary_labelâ€è¿‡æ»¤æŽ‰ï¼Œå¦‚æ­¤å¤„å»ºè®®çš„é‚£æ ·
- é€‰å–2023/2022/2021/2020å¹´ç«žèµ›æ•°æ®+2020å¹´é™„åŠ ç«žèµ›æ•°æ®ä¸­å­˜åœ¨çš„ç‰©ç§ï¼Œä¸”å‰ææ˜¯è¯¥ç‰©ç§çš„ä»£è¡¨æ•°é‡è¶…è¿‡10ä¸ªã€‚æ€»å…±æœ‰822ç§ã€‚
- æ·»åŠ äº†æ¥è‡ª Xeno Canto çš„é€‰å®šç‰©ç§çš„é™„åŠ æ–‡ä»¶ã€‚

## æ³½è¯ºå¤š

æˆ‘é€‰æ‹©äº† nocall åŒºåŸŸå¹¶å°†å…¶ç”¨ä½œèƒŒæ™¯å¢žå¼ºã€‚

## æ— æ•ˆçš„æ•°æ®å®žéªŒ

- å¯¹æ‰€æœ‰ Xeno Canto æ•°æ®è¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒã€‚
- 2021 å¹´ç¬¬äºŒåçš„èƒŒæ™¯å™ªéŸ³ä½œä¸ºèƒŒæ™¯å¢žå¼º
- ESC50 ä½œä¸ºèƒŒæ™¯å¢žå¼º
- ä»Žé™„åŠ æ•°æ®ä¸­ä»…é€‰æ‹©é«˜è´¨é‡æ ·æœ¬ (>=32kHz)
- ä¹Ÿè®¸æˆ‘åˆšåˆšå¿˜è®°äº† 200 å¤šä¸ªå®žéªŒä¸­çš„ä¸€äº›å…¶ä»–æƒ³æ³•

## éªŒè¯ï¼šåƒcomAPä¸€æ ·è½¯ï¼Œä¸è¦åƒF1ä¸€æ ·ç¡¬

æœ€åŽï¼ä¸ŽéŸ³æ™¯æ•°æ®ç›¸æ¯”ï¼Œæˆ‘ä»¬ä¸å¿…åœ¨å®Œå…¨ä¸åŒçš„è®­ç»ƒæ•°æ®ä¸Šé€‰æ‹©é˜ˆå€¼ï¼Œæå‡ºè¶…çº§å¤æ‚çš„æ–¹æ¡ˆæˆ–ä¸‹é™ 19 ä½ï¼ˆå°±åƒæˆ‘åœ¨ 2021 å¹´æ‰€åšçš„é‚£æ ·ï¼‰

æˆ‘ä½¿ç”¨äº†ä¸Žå‰å‡ å¹´çš„æ¯”èµ›å‡ ä¹Žç›¸åŒçš„éªŒè¯æ–¹æ¡ˆï¼š

- ç®€åŽ†åˆ†å±‚ 5 æŠ˜
- éšç€æ—¶é—´çš„æŽ¨ç§»ï¼Œä»Žæ‰€æœ‰æ ·æœ¬çš„æ¯ 5 ç§’å‰ªè¾‘ä¸­èŽ·å–æœ€å¤§æ¦‚çŽ‡
- é‡è¦æç¤ºï¼šå¯¹äºŽå¸¦è¡¬åž«çš„ comAPï¼Œåœ¨æŠ˜å å¤„å–å‡å€¼éžå¸¸é‡è¦ï¼Œè€Œä¸æ˜¯åœ¨æŠ˜å å¤„è¿›è¡Œï¼ï¼ï¼

å½“ç„¶ï¼ŒCVå’ŒLBçš„ç»å¯¹æ•°é‡æ˜¯ä¸åŒçš„ï¼š

æœ€ä½³å…¬å…±LBï¼š0.84444ï¼ˆ4ä¸ªå››:)ï¼‰
æœ€ä½³ç§äººLBï¼š0.76392
æœ€ä½³ç®€åŽ†ï¼š0.9083368282233681

ä½†æŽ’åç›¸å…³æ€§éžå¸¸å¥½ã€‚ CV æ”¹å–„ 0.0 å€ï¼ˆæˆ–æ›´å¤šï¼‰å¯¼è‡´ LB æ”¹å–„ã€‚æˆ‘çš„å®žéªŒå‡ ä¹Žæ‹¥æœ‰æ‰€æœ‰ CV ç»“æžœï¼Œå› æ­¤æˆ‘å¸Œæœ›æœ‰æ—¶é—´å‘è¡¨ä¸€ç¯‡åŒ…å«è¯¦ç»†æ¶ˆèžç ”ç©¶å’Œ CV-LB ç›¸å…³æ€§ç ”ç©¶çš„è®ºæ–‡ã€‚

## è®­ç»ƒ

æˆ‘çœ‹äº†@philippsinger çš„æ¼”ç¤ºï¼Œäº†è§£äº†æˆ‘ä¸€ç›´ä»¥æ¥çš„è¿‡åº¦æ‹Ÿåˆç¨‹åº¦ã€‚

ç”±äºŽæ—¶é—´å’Œè®¾å¤‡çš„é™åˆ¶ï¼Œæˆ‘é€‰æ‹©äº†ä»¥ä¸‹æ–¹æ¡ˆï¼š

- éªŒè¯ CV çš„å‡è®¾å¹¶æäº¤å‰ 2-3 ä¸ªæŠ˜å ã€‚
- å¯¹äºŽå®Œæ•´è®­ç»ƒæ•°æ®çš„é›†æˆé‡æ–°è®­ç»ƒï¼Œå› æ­¤æ¯ä¸ªè®¾ç½®éƒ½æœ‰ä¸€ä¸ªæ¨¡åž‹

åŸ¹è®­è¯¦æƒ…ï¼š

- 50 Epochs
- Adam
- CosineAnnealing from 1e-4 (or 1e-3) to 1e-6
- Focal loss
- 64 BS
- 5 second chunk
- SUPER IMPORTANT: Class sampling weights

```python
sample_weights = (
    all_primary_labels.value_counts() / 
    all_primary_labels.value_counts().sum()
)  ** (-0.5)

```
- Same setups for pretrain and finetune

Stages:

- Pretrain - refer to Pretrained Dataset
- Tune only on scored species

## æ¨¡åž‹
ç”±äºŽè®¡ç®—é™åˆ¶ï¼Œæˆ‘ä»¬æ— æ³•ä½¿ç”¨æ·±åº¦å­¦ä¹ çš„é»„é‡‘æ³•åˆ™ï¼šå †å æ›´å¤šå±‚ï¼

æ‰€ä»¥æˆ‘æ·±å…¥ç ”ç©¶äº†æŽ¨ç†ä¼˜åŒ–æŠ€æœ¯ï¼š

- ONNX - è¿™å¯¹æˆ‘æ¥è¯´æ•ˆæžœå¾ˆå¥½ã€‚å®ƒç¨å¾®ç¼©çŸ­äº†æŽ¨ç†æ—¶é—´ï¼Œå¹¶å…è®¸æˆ‘å‡å°‘æŽ¨ç†ç¬”è®°æœ¬ä¸­è‡ªå®šä¹‰ä¾èµ–é¡¹çš„æ•°é‡ã€‚
é‡åŒ– - æˆ‘èŠ±äº†ä¸€ä¸ªå¤šæ˜ŸæœŸçš„æ—¶é—´æ¥å°è¯•å®ƒï¼Œä½†ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘æ²¡æœ‰æˆåŠŸ:(
- openvino - æˆ‘æ²¡æœ‰ä½¿ç”¨æˆ–å°è¯•è¿™ä¸ªï¼Œæˆ‘åªæ˜¯é˜…è¯»äº†ç¬¬äºŒåçš„æè¿°å¹¶çƒ§æ¯äº†æˆ‘çš„æ¤…å­

æ€»çš„æ¥è¯´ï¼Œæˆ‘çš„æœ€ç»ˆæäº¤æ˜¯ 3 ä¸ªå£°éŸ³äº‹ä»¶æ£€æµ‹ (SED) æ¨¡åž‹çš„é›†åˆï¼Œå…·æœ‰ä»¥ä¸‹ä¸»å¹²ï¼š

- eca_nfnet_l0ï¼ˆ2 é˜¶æ®µè®­ç»ƒï¼›å¯åŠ¨ LR 1e-3ï¼‰
- convnext_small_fb_in22k_ft_in1k_384ï¼ˆ2é˜¶æ®µè®­ç»ƒï¼›å¼€å§‹LR 1e-4ï¼‰
- convnextv2_tiny_fcmae_ft_in22k_in1k_384ï¼ˆ1é˜¶æ®µè®­ç»ƒï¼›å¼€å§‹LR 1e-4ï¼‰

è°ƒæ•´ä¸åŒæž¶æž„çš„èµ·å§‹å­¦ä¹ çŽ‡éžå¸¸é‡è¦ï¼

## å¢žå¼º

æˆ‘å¯¹å¢žå¼ºé€‰æ‹©éžå¸¸æŒ‘å‰”ï¼Œæ‰€ä»¥æˆ‘çš„æœ€ç»ˆæ¨¡åž‹ä½¿ç”¨äº†ä¸‹ä¸€ä¸ªï¼š

- Mixup : Simply OR Mixup with Prob = 0.5
- BackgroundNoise with Zenodo nocall
- RandomFiltering - a custom augmentation: in simple terms, it's a simplified random Equalizer
- Spec Aug:
  - Freq:
    - Max length: 10
    - Max lines: 3
    - Probability: 0.3
  - Time:
    - Max length: 20
    - Max lines: 3
    - Probability: 0.3

## å°æŽ¨ç†æŠ€å·§
- ä½¿ç”¨æ¸©åº¦å¹³å‡å€¼ï¼š `pred = (pred**2).mean(axis=0) ** 0.5`
- ä½¿ç”¨æ³¨æ„åŠ› SED æ¦‚çŽ‡ * 0.75 + æœ€å¤§æ—¶é—´æ¦‚çŽ‡ * 0.25

æ‰€æœ‰è¿™äº›éƒ½å¸¦æ¥äº†å¾®å°çš„æ”¹è¿›ï¼Œä½†è¿™åªæ˜¯å‰ä¸‰åçš„é—®é¢˜:)

## å…¶ä»–äº§ç”Ÿç¢³è¶³è¿¹ä½†æ²¡æœ‰æé«˜æˆ‘çš„ LB åˆ†æ•°çš„ä¸œè¥¿

æœ¬èŠ‚è¿˜è¿œæœªå®Œæˆï¼Œä½†è®©æˆ‘ä»¬æ·»åŠ ä¸€äº›æˆ‘çŽ°åœ¨æƒ³åˆ°çš„å†…å®¹ï¼š

- 2021 å¹´ç¬¬äºŒå[æ–¹æ¡ˆ](https://www.kaggle.com/competitions/birdclef-2023/discussion/412707)ã€‚æˆ‘å·²ç»å°è¯•è¿‡ï¼ˆå°±åƒæˆ‘åœ¨ 2022 å¹´æ‰€åšçš„é‚£æ ·ï¼‰ï¼Œä½†ä¸å¹¸çš„æ˜¯å®ƒå¯¹æˆ‘ä¸èµ·ä½œç”¨
- å¯¹æ•´ä¸ª Xeno Canto è¿›è¡Œé¢„è®­ç»ƒ
- è®­ç»ƒæ›´å¤§çš„å—ã€‚å¦‚æžœæˆ‘æŽ¨æ–­è¾ƒå°çš„å—æˆ–ç›¸åŒé•¿åº¦çš„å—ï¼Œåˆ™ä¼šå‡ºçŽ°ç›¸åŒçš„ç»“æžœ
- å½©è‰²å™ªå£°å¢žå¼º
- CQT æˆ–å¶
- å…·ä½“å¾®è°ƒï¼šè¾ƒå°çš„ LRã€è¾ƒå°çš„ epoch æ•°é‡ã€å†»ç»“ä¸»å¹²ã€ä¸»å¹²å’Œå¤´éƒ¨çš„ä¸åŒ LR
- æ³¨æ„åŠ› SED æ¦‚çŽ‡æŸå¤± + æœ€å¤§æ—¶é—´æ¦‚çŽ‡æŸå¤±
- æ·±åº¦ç£å¯¼
- MixUp çš„ä¸åŒ alpha
- å˜åŽ‹å™¨æž¶æž„ã€‚ä¾‹å¦‚ ECAPA TDNN

## ä»£ç æ–¹æ¡ˆ

```python
!nvidia-smi
```

    /bin/bash: nvidia-smi: command not found




```python
!pip install /kaggle/input/bird-clef-2023-addones/onnxruntime-1.14.0-cp37-cp37m-manylinux_2_27_x86_64.whl --no-deps
```

    Processing /kaggle/input/bird-clef-2023-addones/onnxruntime-1.14.0-cp37-cp37m-manylinux_2_27_x86_64.whl
    
    Installing collected packages: onnxruntime
    
    Successfully installed onnxruntime-1.14.0
    
    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv[0m[33m
    
    [0m[33mWARNING: There was an error checking the latest version of pip.[0m[33m
    
    [0m


```python
!pip list | grep onnx
```

    onnx                                   1.13.1
    
    onnxruntime                            1.14.0
    
    [33mWARNING: There was an error checking the latest version of pip.[0m[33m
    
    [0m


```python
import sys
sys.path.append('/kaggle/input/bird-clef-2023-code/main_folder/main_folder/')
```


```python
import pandas as pd
import numpy as np
import librosa
import seaborn as sns
import os
import json
import IPython.display as ipd
import soundfile as sf
import torch
import h5py
import onnxruntime as ort

from glob import glob
from tqdm import tqdm
from matplotlib import pyplot as plt
from itertools import chain
from os.path import join as pjoin
from copy import deepcopy


from code_base.models import WaveCNNClasifier, WaveCNNAttenClasifier
from code_base.datasets import WaveDataset, WaveAllFileDataset
from code_base.utils.inference_utils import apply_avarage_weights_on_swa_path
from code_base.inefernce import BirdsInference
from code_base.utils import load_json, compose_submission_dataframe, groupby_np_array, stack_and_max_by_samples
from code_base.utils.metrics import padded_cmap_numpy
%matplotlib inline

```

    `speechbrain` was not imported
    `nnAudioSTFT` was not imported
    `noisereduce` was not imported


# Config


```python
EXP_NAME = "convnext_small_fb_in22k_ft_in1k_384__convnextv2_tiny_fcmae_ft_in22k_in1k_384__eca_nfnet_l0_noval_v32_075Clipwise025TimeMax_GausMean"
TRAIN_PERIOD = 5
print("Possible checkpoints:\n\n{}".format("\n".join(set([
    os.path.basename(el) for el in glob(f"/kaggle/input/bird-clef-2023-models/{EXP_NAME}/{EXP_NAME}/*/checkpoints/*.pt*") if "train" not in os.path.basename(el)
]))))
```

    Possible checkpoints:


â€‹    
â€‹    


```python
CONFIG = {
    # Main
    "run_validation": False,
    "run_test": True,
    # Inference Class
    "use_sigmoid": False,
    # Data config
#     "train_df_path":"/home/vova/data/exps/BirdCLEF_2023/birdclef_2023/train_metadata_extended.csv",
#     "split_path":"/home/vova/data/exps/BirdCLEF_2023/cv_split_2023_v1.npy",
    "folds":[0],
#     "train_data_root":"/home/vova/data/exps/BirdCLEF_2023/birdclef_2023/train_audio/",
#     "test_data_root": "/kaggle/input/bird-clef-2023-addones/fake_test_20/fake_test_20/*.ogg",
    "test_data_root": "/kaggle/input/birdclef-2023/test_soundscapes/*.ogg",
    "label_map_data_path":'/kaggle/input/bird-clef-2023-models/bird2int_2023.json',
#     "label_map_data_path":'/kaggle/input/bird-clef-2023-models/bird221int_202x.json',
#     "label_map_data_path": "/kaggle/input/bird-clef-2023-models/xc_birds_202x_only_scored.json",
#     "label_map_data_path": '/kaggle/input/bird-clef-2023-models/bird2id_xc_pretrain.json',
#     "label_map_data_path": "/kaggle/input/bird-clef-2023-models/bird2id.json",
#     "label_map_data_path":"/kaggle/input/bird-clef-2023-models/bird2id_v1.json",
    "lookback":None,
    "lookahead":None,
    "segment_len":5,
    "step": None,
    "late_normalize": True,
    # Model config
    "exp_name":EXP_NAME,
#     "model_class": WaveCNNAttenClasifier,
#     "model_config": dict(
#         backbone="convnext_tiny_in22ft1k",
#         mel_spec_paramms={
#             "sample_rate": 32000,
#             "n_mels": 128,
#             "f_min": 20,
#             "n_fft": 2048,
#             "hop_length": 512,
#             "normalized": True,
#         },
#         head_config={
#             "p": 0.5,
#             "num_class": 264,
#             "train_period": TRAIN_PERIOD,
#             "infer_period": TRAIN_PERIOD,
#         },
#         pretrained=False
#     ),
#     "chkp_name":"model.last.pth",
#     "swa_checkpoint": None,
#     "distributed_chkp": False,
}

if CONFIG.get("use_sed_mode", False):
    assert CONFIG["step"] is not None
else:
    assert CONFIG["step"] is None
    
if "folds" not in CONFIG:
    CONFIG["folds"] = list(range(CONFIG["n_folds"]))
```

# Data


```python
bird2id = load_json(CONFIG["label_map_data_path"])
```


```python
if CONFIG["run_validation"]:
    df = pd.read_csv(CONFIG["train_df_path"])
    split = np.load(CONFIG["split_path"], allow_pickle=True)
    val_df = [df.iloc[split[fold_id][1]].reset_index(drop=True) for fold_id in CONFIG["folds"]]
```


```python
if CONFIG["run_test"]:
    test_au_pathes = glob(CONFIG["test_data_root"])

    test_df = pd.DataFrame({
        "filename": test_au_pathes,
        "duration_s": [librosa.get_duration(filename=el) for el in test_au_pathes]
    })
```

    /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.
    	This alias will be removed in version 1.0.


â€‹    


```python
if CONFIG["run_validation"]:
    val_ds_conig = {
       "root": CONFIG["train_data_root"],
       "label_str2int_mapping_path": CONFIG["label_map_data_path"],
       "use_audio_cache": True,
       "n_cores": 64,
       "verbose": False,
       "segment_len": CONFIG["segment_len"],
       "lookback":CONFIG["lookback"],
       "lookahead":CONFIG["lookahead"],
       "sample_id": None,
       "late_normalize": CONFIG["late_normalize"],
       "step": CONFIG["step"],
       "validate_sr": 32_000
    }
if CONFIG["run_test"]:
    ds_config_test = {
       "root": "",
       "label_str2int_mapping_path": CONFIG["label_map_data_path"],
       "n_cores": 64,
       "use_audio_cache": True,
       "test_mode": True,
       "segment_len": CONFIG["segment_len"],
       "lookback":CONFIG["lookback"],
       "lookahead":CONFIG["lookahead"],
        "sample_id": None,
        "late_normalize": CONFIG["late_normalize"],
        "step": CONFIG["step"],
        "validate_sr": 32_000
    }
loader_config = {
    "batch_size": 4,
    "drop_last": False,
    "shuffle": False,
    "num_workers": 0,
}
```


```python
if CONFIG["run_test"]:
    ds_test = WaveAllFileDataset(df=test_df, **ds_config_test)
if CONFIG["run_validation"]:
    ds_val = [WaveAllFileDataset(df=df, **val_ds_conig) for df in val_df]
```

    secondary_labels is not found in df. Maybe test or nocall mode



```python
if CONFIG["run_validation"]:
    loader_val = [torch.utils.data.DataLoader(
        ds,
        **loader_config,
    )for ds in ds_val]
if CONFIG["run_test"]:
    loader_test = torch.utils.data.DataLoader(
        ds_test,
        **loader_config,
    )
```



# Model


```python
def create_model_and_upload_chkp(
    model_class,
    model_config,
    model_device,
    model_chkp,
    use_distributed=False,
    swa_checkpoint=None
):
    print(model_chkp)
    if "swa" in model_chkp:
        print("swa by {}".format(os.path.splitext(os.path.basename(model_chkp))[0]))
        t_chkp = apply_avarage_weights_on_swa_path(model_chkp, use_distributed=use_distributed, take_best=swa_checkpoint)
    else:
        print("vanilla model")
        t_chkp = torch.load(model_chkp, map_location="cpu")
        
    t_model = model_class(**model_config, device=model_device)
    t_model.load_state_dict(t_chkp)
    t_model.eval()
    return t_model
```
è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªå‡½æ•°`create_model_and_upload_chkp`ï¼Œè¯¥å‡½æ•°çš„ä½œç”¨æ˜¯åˆ›å»ºæ¨¡åž‹å¹¶åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶ã€‚ä»¥ä¸‹æ˜¯å¯¹ä»£ç çš„åˆ†æžï¼š

1. å‡½æ•°æŽ¥å—ä»¥ä¸‹å‚æ•°ï¼š
   - `model_class`: æ¨¡åž‹ç±»åˆ«ï¼Œç”¨äºŽåˆ›å»ºæ¨¡åž‹çš„ç±»ã€‚
   - `model_config`: åŒ…å«æ¨¡åž‹é…ç½®ä¿¡æ¯çš„å­—å…¸ã€‚
   - `model_device`: æŒ‡å®šæ¨¡åž‹æ‰€åœ¨çš„è®¾å¤‡ã€‚
   - `model_chkp`: æ¨¡åž‹çš„æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ã€‚
   - `use_distributed`: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒã€‚é»˜è®¤ä¸º`False`ã€‚
   - `swa_checkpoint`: å¦‚æžœæ¨¡åž‹ä½¿ç”¨äº†SWAï¼ˆStochastic Weight Averagingï¼‰æ–¹æ³•ï¼Œæä¾›SWAæ£€æŸ¥ç‚¹çš„è·¯å¾„ã€‚

2. æ‰“å°æ¨¡åž‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼šå‡½æ•°ä¼šæ‰“å°å‡ºä¼ å…¥çš„`model_chkp`å‚æ•°çš„å€¼ã€‚

3. åˆ¤æ–­æ˜¯å¦æ˜¯ SWA æ¨¡åž‹ï¼šé€šè¿‡æ£€æŸ¥`model_chkp`æ˜¯å¦åŒ…å« "swa" æ¥ç¡®å®šæ˜¯å¦ä½¿ç”¨ SWAã€‚å¦‚æžœæ˜¯ SWA æ¨¡åž‹ï¼Œåˆ™æ‰“å°ç›¸åº”çš„ä¿¡æ¯ï¼Œå¹¶è°ƒç”¨`apply_avarage_weights_on_swa_path`å‡½æ•°æ¥åº”ç”¨ SWA æ–¹æ³•çš„å¹³å‡æƒé‡ã€‚å¦åˆ™ï¼Œæ‰“å° "vanilla model" è¡¨ç¤ºæ™®é€šæ¨¡åž‹ï¼Œå¹¶ä½¿ç”¨`torch.load`åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶ã€‚

4. åˆ›å»ºæ¨¡åž‹å®žä¾‹ï¼šä½¿ç”¨`model_class`å’Œ`model_config`åˆ›å»ºä¸€ä¸ªæ¨¡åž‹å®žä¾‹`t_model`ï¼ŒåŒæ—¶æŒ‡å®šè®¾å¤‡ä¸º`model_device`ã€‚

5. åŠ è½½æ¨¡åž‹å‚æ•°ï¼šä½¿ç”¨åŠ è½½çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸­çš„æƒé‡å‚æ•°è°ƒç”¨`t_model.load_state_dict(t_chkp)`åŠ è½½æ¨¡åž‹å‚æ•°ã€‚

6. è®¾ç½®æ¨¡åž‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼šé€šè¿‡è°ƒç”¨`t_model.eval()`å°†æ¨¡åž‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ã€‚

7. è¿”å›žæ¨¡åž‹å®žä¾‹ï¼šè¿”å›žåˆ›å»ºå¹¶åŠ è½½äº†æ£€æŸ¥ç‚¹çš„æ¨¡åž‹å®žä¾‹`t_model`ã€‚

æ€»ä½“è€Œè¨€ï¼Œè¿™ä¸ªå‡½æ•°çš„ä¸»è¦ä½œç”¨æ˜¯æ ¹æ®ç»™å®šçš„å‚æ•°åˆ›å»ºæ¨¡åž‹å®žä¾‹ï¼Œå¹¶æ ¹æ®æä¾›çš„æ£€æŸ¥ç‚¹æ–‡ä»¶åŠ è½½æ¨¡åž‹çš„æƒé‡å‚æ•°ã€‚å¦‚æžœæ£€æŸ¥ç‚¹æ–‡ä»¶åä¸­åŒ…å« "swa"ï¼Œåˆ™ä¼šåº”ç”¨ SWA æ–¹æ³•çš„å¹³å‡æƒé‡ã€‚æœ€ç»ˆè¿”å›žåŠ è½½äº†æƒé‡å‚æ•°çš„æ¨¡åž‹å®žä¾‹ã€‚

```python
# model = [create_model_and_upload_chkp(
#         model_class=CONFIG["model_class"],
#         model_config=CONFIG['model_config'],
#         model_device="cpu",
#         model_chkp=f"/kaggle/input/bird-clef-2023-models/{CONFIG['exp_name']}/{CONFIG['exp_name']}/fold_{m_i}/checkpoints/{CONFIG['chkp_name']}",
#         swa_checkpoint=CONFIG['swa_checkpoint'],
#         use_distributed=CONFIG['distributed_chkp']
# ) for m_i in CONFIG["folds"]]
```


```python
model = ort.InferenceSession(f"/kaggle/input/bird-clef-2023-models/{CONFIG['exp_name']}/{CONFIG['exp_name']}/checkpoints/model_simpl.onnx")
```

# Inference Class


```python
inference_class = BirdsInference(
    device="cpu",
    verbose_tqdm=True,
    use_sigmoid=CONFIG["use_sigmoid"],
#     model_output_key=CONFIG["model_output_key"],
)
```

# Val Predict


```python
if CONFIG["run_validation"]:
    val_tgts, val_preds, val_preds_long = inference_class.predict_val_loaders(
        nn_models=model,
        data_loaders=loader_val
    )
    print(
        f"Min prob {val_preds.min()}. Max prob {val_preds.max()}.\n"
        f"Padded CMAP: {padded_cmap_numpy(val_tgts, val_preds)}"
    )
```

# Test Pred


```python
if CONFIG["run_test"]:
    test_preds, test_preds_long, test_dfidx, test_end = inference_class.predict_test_loader(
        nn_models=model,
        data_loader=loader_test,
        is_onnx_model=True
    )
    test_pred_df = compose_submission_dataframe(
        probs=test_preds,
        dfidxs=test_dfidx,
        end_seconds=test_end,
        filenames=loader_test.dataset.df[loader_test.dataset.name_col].copy(),
        bird2id=bird2id,
    )
    sample_submission = pd.read_csv("/kaggle/input/birdclef-2023/sample_submission.csv")
    if test_pred_df.shape[1] > sample_submission.shape[1]:
        print("Shrinking columns")
        test_pred_df = test_pred_df[sample_submission.columns]
```

      0%|          | 0/30 [00:00<?, ?it/s]
    
    Loading /kaggle/input/birdclef-2023/test_soundscapes/soundscape_29201.ogg to audio cache


    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:49<00:00,  1.65s/it]



```python
test_pred_df.iloc[:,1:].values.max(), test_pred_df.iloc[:,1:].values.min()
```




    (0.8837069272994995, 0.004084157757461071)




```python
plt.hist(test_pred_df.iloc[:,1:].values.max(axis=1), bins=30);
```


â€‹    
![png](https://www.kaggleusercontent.com/kf/130572858/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..SHeM5U5kHKJQqmIz2MxOdw.Jv7dq963obw-94-6DgQTosF1St_osn-rY5qn8YajGzWqiFQrMBag_3vmZIgGFzymx2AsaReJHG9m2nrTbh1ittJ7krxrPYCDLp5730SMIJCBvsf0cO8JHCUz_Gueba1sG3IebiUzQx0axaJ7I7q3SFSGgiEy3IJnYfnsTnSf1u4BPR6QDfF8VuRQ4TxzRZoL_2afFUY4BD_dTrfhGqjbfGwzkmZCMAcEh-gEPIHJLt5-90du4FkuGXXWFSPQp9K-f4aD0IS85qBFbUbzfO8BOmlN_074yChJGL6riJWwH8zFPbJNQ-xq44ijyBdCIUsXzrtDTLMfRBCrZCuUe09GGPG_ePXl7Q_HHvFEO48Dd74z0dcHDu4bXpYNVB18coLIjCRuh6p56qHGhVhXIV0qaipskoMFG9fLkoTDkRx6cSbyM4PuCr3lDmYvnPxQSloz7Ym2JuxcwnIn_uJNVafMTAAQTRqofTerhDvsdU_xl0hiSavv-WaM6hqUDNbsF1LOI0N5HmLBWbZ-M1kx3yaovuGbP8Ki8VxDE_8T24J5CTYMYwZhzDcE3mqcsVJN74G8O8PlK6NtrIR3qx5d0TdOqPt5RxLAdhwk8ihzquRYO_nidDkFRSURjjwjI8FbsmxoLrI47vn7s7CS6c-3uTw2jnKw1c8_KCNklPAmvSRz7FKZQ5tftdb_fTBKgys1X5Bs.SopSi2l7sImJ59xNisRoiQ/__results___files/__results___26_0.png)
â€‹    
è¿™æ®µä»£ç æ˜¯ä¸€ä¸ªæ¡ä»¶è¯­å¥ï¼Œæ ¹æ®`CONFIG["run_test"]`çš„å€¼æ‰§è¡Œä¸åŒçš„æ“ä½œã€‚ä»¥ä¸‹æ˜¯å¯¹ä»£ç çš„åˆ†æžï¼š

1. `CONFIG`æ˜¯ä¸€ä¸ªé…ç½®å­—å…¸ï¼Œå…¶ä¸­åŒ…å«äº†ç¨‹åºçš„ä¸€äº›é…ç½®å‚æ•°ã€‚

2. `CONFIG["run_test"]`æ˜¯ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦è¿è¡Œæµ‹è¯•ã€‚å¦‚æžœ`CONFIG["run_test"]`ä¸º`True`ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼›å¦åˆ™ï¼Œè·³è¿‡è¿™æ®µä»£ç å—ã€‚

3. è°ƒç”¨`inference_class.predict_test_loader`æ–¹æ³•è¿›è¡Œæµ‹è¯•é›†æŽ¨æ–­ï¼ˆinferenceï¼‰ï¼š
   - `nn_models`: ä½¿ç”¨çš„ç¥žç»ç½‘ç»œæ¨¡åž‹ã€‚
   - `data_loader`: æµ‹è¯•é›†çš„æ•°æ®åŠ è½½å™¨ã€‚
   - `is_onnx_model`: ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ¨¡åž‹æ˜¯å¦ä¸ºONNXæ ¼å¼ã€‚

4. è°ƒç”¨`compose_submission_dataframe`æ–¹æ³•ï¼š
   - `probs`: æµ‹è¯•é›†çš„é¢„æµ‹æ¦‚çŽ‡ã€‚
   - `dfidxs`: æµ‹è¯•é›†ç´¢å¼•ã€‚
   - `end_seconds`: æµ‹è¯•é›†ä¸­æ¯ä¸ªæ ·æœ¬çš„ç»“æŸæ—¶é—´ã€‚
   - `filenames`: æµ‹è¯•é›†æ–‡ä»¶åã€‚
   - `bird2id`: é¸Ÿç±»åˆ«åˆ°IDçš„æ˜ å°„ã€‚

5. è¯»å–æ ·æœ¬æäº¤æ–‡ä»¶ï¼ˆsample_submission.csvï¼‰ï¼š
   - ä½¿ç”¨`pd.read_csv`è¯»å–è·¯å¾„ä¸º"/kaggle/input/birdclef-2023/sample_submission.csv"çš„æ ·æœ¬æäº¤æ–‡ä»¶ï¼Œå°†å…¶å­˜å‚¨åœ¨`sample_submission`å˜é‡ä¸­ã€‚

6. æ£€æŸ¥æµ‹è¯•é¢„æµ‹æ•°æ®æ¡†çš„åˆ—æ•°æ˜¯å¦å¤§äºŽæ ·æœ¬æäº¤æ–‡ä»¶çš„åˆ—æ•°ï¼š
   - å¦‚æžœ`test_pred_df`çš„åˆ—æ•°ï¼ˆç‰¹å¾æ•°ï¼‰å¤§äºŽ`sample_submission`çš„åˆ—æ•°ï¼Œåˆ™æ‰“å° "Shrinking columns" å¹¶å°†`test_pred_df`çš„åˆ—æ•°æˆªæ–­ä¸ºä¸Ž`sample_submission`ç›¸åŒçš„åˆ—æ•°ã€‚

æ€»ä½“è€Œè¨€ï¼Œè¿™æ®µä»£ç å—çš„ä½œç”¨æ˜¯åœ¨è¿è¡Œæµ‹è¯•æ—¶ï¼Œè¿›è¡Œæµ‹è¯•é›†çš„æŽ¨æ–­ï¼ˆinferenceï¼‰ï¼Œç”Ÿæˆæäº¤çš„æ•°æ®æ¡†ï¼Œå¹¶åœ¨å¿…è¦æ—¶æˆªæ–­åˆ—ä»¥ç¬¦åˆæ ·æœ¬æäº¤æ–‡ä»¶çš„æ ¼å¼ã€‚

# Map Predictions


```python
if CONFIG.get("check_inf_class", False):
    val_ds_check = WaveAllFileDataset(df=val_df[0], **{
           "root": CONFIG["train_data_root"],
           "label_str2int_mapping_path": CONFIG["label_map_data_path"],
           "n_cores": 64,
           "use_audio_cache": True,
           "test_mode": True,
           "segment_len": CONFIG["segment_len"],
           "lookback":CONFIG["lookback"],
           "lookahead":CONFIG["lookahead"],
            "sample_id": None,
            "late_normalize": CONFIG["late_normalize"],
            "step": CONFIG["step"],
        }
    )
    val_loader_check = torch.utils.data.DataLoader(
        val_ds_check,
        **loader_config
    )
    
    test_preds_check, test_preds_long_check, test_dfidx_check, test_end_check = inference_class.predict_test_loader(
        nn_models=model,
        data_loader=val_loader_check
    )
    test_preds_check_grouped = groupby_np_array(
        groupby_f=test_dfidx_check,
        array_to_group=test_preds_check,
        apply_f=stack_and_max_by_samples,
    )
    print(np.allclose(
        test_preds_check_grouped,
        val_preds
    ))
```

# Boost Probs


```python
# CLASSES = list(set(test_pred_df.columns[1:]))
# print(len(CLASSES))
```


```python
# def connected_region_indices(bool_array):
#     connected_regions = []
#     region = []

#     for i, value in enumerate(bool_array):
#         if value:
#             region.append(i)
#         elif region:
#             connected_regions.append(region)
#             region = []

#     if region:
#         connected_regions.append(region)

#     return connected_regions

# def modify_probabilities(group, lower_prob=0.5, max_prob=0.75, min_seq_len=2):
#     class_cols = ['shesta1', 'colsun2', 'amesun2', 'bcbeat1', 'marsun2']
    
#     mask_low = (group[CLASSES] > lower_prob).values
#     mask_high = (group[CLASSES] > max_prob).values
    
#     # At least 2 chunks AND exceeds max_prob
#     classes_to_boost = np.where((mask_low.sum(axis=0) >= min_seq_len) & mask_high.any(axis=0))[0]
#     if len(classes_to_boost) > 0:
#         for cls in classes_to_boost:
#             new_probs = group[CLASSES[cls]].values.copy()
#             connected_regions = connected_region_indices(mask_low[:,cls])
#             for region in connected_regions:
#                 if len(region) >= min_seq_len:
#                     max_region_prob = group[CLASSES[cls]].iloc[region].max()
#                     if max_region_prob > max_prob:
#                         # print(f"Boosting: {CLASSES[cls]}")
#                         new_probs[region] = max_region_prob
#             group[CLASSES[cls]] = new_probs
                
#     return group
```


```python
# test_pred_df["id"] = test_pred_df["row_id"].apply(lambda x: "_".join(x.split("_")[:-1]))
# test_pred_df["sec"] = test_pred_df["row_id"].apply(lambda x: int(x.split("_")[-1]))

# test_pred_df = test_pred_df.sort_values(["id", "sec"]).reset_index(drop=True)

# test_pred_df = test_pred_df.groupby("id").apply(modify_probabilities).reset_index(drop=True)

# test_pred_df = test_pred_df.drop(columns=["id", "sec"])
```


```python
# np.where((test_pred_df_new[CLASSES].values != test_pred_df[CLASSES].values).sum(axis=0))
# test_pred_df_new.loc[(test_pred_df_new[CLASSES[185]] - test_pred_df[CLASSES[185]]) > 0, CLASSES[185]]
# test_pred_df.loc[(test_pred_df_new[CLASSES[185]] - test_pred_df[CLASSES[185]]) > 0, CLASSES[185]]
```


```python
# def boost_pred_prob(
#     input_df,
#     class_columns,
#     select_prob=0.8,
#     boost_prob=0.2
# ):
#     result_df = input_df.copy()
#     for sample_id in tqdm(set(input_df["id"])):
#         sample_id_pred_df = result_df.loc[result_df["id"] == sample_id, class_columns]
#         boost_classes = class_columns[sample_id_pred_df.values.max(axis=0) > select_prob]
#         boost_mask = sample_id_pred_df[boost_classes].values < select_prob - boost_prob
#         result_df.loc[result_df["id"] == sample_id, boost_classes] = (
#             (boost_mask.astype(np.float32) * (sample_id_pred_df[boost_classes].values + boost_prob)) +
#             ((~boost_mask).astype(np.float32) * sample_id_pred_df[boost_classes].values)
#         )
#     return result_df

# test_pred_df["id"] = test_pred_df["row_id"].apply(lambda x: "_".join(x.split("_")[:-1]))
# test_pred_df["sec"] = test_pred_df["row_id"].apply(lambda x: int(x.split("_")[-1]))

# test_pred_df = boost_pred_prob(
#     input_df=test_pred_df,
#     class_columns=test_pred_df.columns[1:-2]
# )
# test_pred_df = test_pred_df.drop(columns=["id", "sec"])
```


```python
# test_pred_df.iloc[:,1:].values.max(), test_pred_df.iloc[:,1:].values.min()
```


```python
# plt.hist(test_pred_df.iloc[:,1:].values.max(axis=1), bins=30);
```

# Save Prediction


```python
sample_submission = pd.read_csv("/kaggle/input/birdclef-2023/sample_submission.csv")
assert set(sample_submission.columns) == set(test_pred_df.columns)
test_pred_df = test_pred_df[sample_submission.columns]
test_pred_df.to_csv("submission.csv", index=False)
```


```python

```